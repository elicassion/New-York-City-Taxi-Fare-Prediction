{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\DS_WORKSPACE\\NYC_TEXI\\venv\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\DS_WORKSPACE\\NYC_TEXI\\venv\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n  from numpy.core.umath_tests import inner1d\nF:\\DS_WORKSPACE\\NYC_TEXI\\venv\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\DS_WORKSPACE\\NYC_TEXI\\venv\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\DS_WORKSPACE\\NYC_TEXI\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import codecs\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "\n",
    "DATA_DIR = 'washed'\n",
    "FILE_NAME = 'train_wfd.csv'\n",
    "INPUT_FILE_PATH = os.path.join(DATA_DIR, FILE_NAME)\n",
    "\n",
    "COLUMN_NAMES = ['distance', 'pickup_longitude', \n",
    "                'dropoff_longitude', 'pickup_hour', 'pickup_year']\n",
    "\n",
    "CHUNKSIZE = 10000000\n",
    "\n",
    "JFK_RANGE = (-73.8250, -73.7746, 40.6397, 40.7121)\n",
    "\n",
    "def get_jfk_mask(d):\n",
    "    return (d.pickup_longitude >= JFK_RANGE[0]) & (d.pickup_longitude <= JFK_RANGE[1]) & \\\n",
    "           (d.pickup_latitude >= JFK_RANGE[2]) & (d.pickup_latitude <= JFK_RANGE[3]) | \\\n",
    "           (d.dropoff_longitude >= JFK_RANGE[0]) & (d.dropoff_longitude <= JFK_RANGE[1]) & \\\n",
    "           (d.dropoff_latitude >= JFK_RANGE[2]) & (d.dropoff_latitude <= JFK_RANGE[3])\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE_PATH, nrows=CHUNKSIZE)\n",
    "df = df.assign(distance=((df.pickup_longitude-df.dropoff_longitude).pow(2) + (df.pickup_latitude-df.dropoff_latitude).pow(2)).pow(0.5))\n",
    "df = df.assign(pickup_hour=(df.pickup_daytime/3600).round())\n",
    "\n",
    "testdf = pd.read_csv('washed\\\\test_fd.csv')\n",
    "testdf = testdf.assign(distance=((testdf.pickup_longitude-testdf.dropoff_longitude).pow(2) + (testdf.pickup_latitude-testdf.dropoff_latitude).pow(2)).pow(0.5))\n",
    "testdf = testdf.assign(pickup_hour=(testdf.pickup_daytime/3600).round())\n",
    "\n",
    "# jfkmask = get_jfk_mask(testdf)\n",
    "\n",
    "# print (np.sum(jfkmask))\n",
    "\n",
    "# exit(1)\n",
    "\n",
    "X = df[COLUMN_NAMES]\n",
    "y = df.fare_amount\n",
    "\n",
    "train_in, test_in, train_f, test_f = sklearn.model_selection.train_test_split(\n",
    "    X, y, train_size=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predicted(d, modelname, datasize):\n",
    "    tstring = str(time.time())\n",
    "    d.to_csv('predicted\\\\{}_predicted.csv'.format(tstring), index=False)\n",
    "    with open('predicted\\\\{}_feature.txt'.format(tstring), 'w') as f:\n",
    "        f.write(', '.join(COLUMN_NAMES)+\n",
    "                '\\n{} data points used.'.format(datasize)+\n",
    "                '\\n{} model'.format(modelname))\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7687661903757836\n"
     ]
    }
   ],
   "source": [
    "lr = sklearn.linear_model.LinearRegression()\n",
    "lr.fit(train_in, train_f)\n",
    "# pY = lr.predict(test_in)\n",
    "s = lr.score(test_in, test_f)\n",
    "print (s)\n",
    "pY = lr.predict(testdf[COLUMN_NAMES])\n",
    "\n",
    "predicted = testdf[['key']].copy()\n",
    "predicted = predicted.assign(fare_amount=pd.Series(pY))\n",
    "\n",
    "write_predicted(predicted, 'simplest linear regression', CHUNKSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7761509513971669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polyr = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "poly_featurizer = PolynomialFeatures(degree=2)\n",
    "\n",
    "train_poly = poly_featurizer.fit_transform(train_in)\n",
    "test_poly = poly_featurizer.transform(test_in)\n",
    "polyr.fit(train_poly, train_f)\n",
    "poly_s = polyr.score(test_poly, test_f)\n",
    "print (poly_s)\n",
    "\n",
    "pred_poly = poly_featurizer.fit_transform(testdf[COLUMN_NAMES])\n",
    "poly_pY = polyr.predict(pred_poly)\n",
    "\n",
    "poly_predicted = testdf[['key']].copy()\n",
    "poly_predicted = poly_predicted.assign(fare_amount=pd.Series(poly_pY))\n",
    "write_predicted(poly_predicted, 'polynomial regression', CHUNKSIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "xgboost_reg = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "xgboost_reg.fit(train_in, train_f)\n",
    "mse = mean_squared_error(test_f, xgboost_reg.predict(test_in))\n",
    "print(\"MSE: {}\".format(mse))\n",
    "\n",
    "xg_pY = xgboost_reg.predict(testdf[COLUMN_NAMES])\n",
    "xg_predicted = testdf[['key']].copy()\n",
    "xg_predicted = xg_predicted.assign(fare_amount=pd.Series(xg_pY))\n",
    "write_predicted(xg_predicted, 'xgboost regression', CHUNKSIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
